
Having 100K stars on GitHub isn't cool.  You know what's cool?  Fixing bugs in your shitty software.

So I have this script I wrote to take a screenshot of my desktop every 10 seconds and upload it to Amazon S3.  I noticed after a while that it was consuming a huge amount of memory.  I traced this to a memory leak in the boto library:  <stackoverflow post>.  I ended up switching to another lib, which worked perfectly, and my process has been trucking along in the background ever since.  I was happy that my script worked, but annoyed that I wasted a couple of hours learning an interface only to abandon it.  And really, I got off easy here, since this was such a small project.  What if a company were to build an entire product on some open source library only to discover way down the road that it has some critical flaw that they need to deal with?

I thought about how to avoid this issue in the future.  What if there was some way to run some basic metrics on a github repo to check for any red flags before investing in it too deeply?  I made a list of repos that had worked well for me in the past, and another those that gave me grief.  I found that the ratio of stars to open issues was strongly predictive of how well the project would work.  I ended up writing a script to do some simple number crunching.  I found this useful, and eventually that script turned into RepoQuality.com.

Out of curiosity, I pulled down several hundred of the most starred projects on GitHub to see how my simple algorithm would rate them.  Here are some notable results:

lodash is #1.  As of January 12, 2016, it has zero open issues, which is amazing for a project of such popularity.

Comparing web frameworks puts something called HapiJS, which I've never heard of, at #1.  Interestingly, Meteor, which has significantly more stars than any of its competitors, ranks pretty far down.

I also found this tool useful in conjunction with [this project](http://www.yasiv.com/github/#/), which takes a repo and finds similar repos by looking at which repos have been starred by the same people.  Using these two apps helped me pick a python unit testing library to use for a project at work and gave me the data to justify that decision to my peers.  Any time I need to decide between several similar libraries, I will use this tool to help me decide.

Soon I want to add the ability for anyone to make their own such lists.  Hopefully other people will find this project as useful as I do.

Interested?  Give me your email and I'll let you know when any new features are released:  <mailing list signup>